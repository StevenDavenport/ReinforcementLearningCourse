{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n",
        "\n",
        "# Chapter 01 — Why Reinforcement Learning Matters **Now**:\n",
        "<hr>\n",
        "\n",
        "**Theme**: RL as *decision-making under uncertainty* — when you can’t just label the right answer.\n",
        "\n",
        "This course is notebook-first, where each chapter is represented as a notebook. Each notebook is designed to be read top-to-bottom and run as you go.\n",
        "\n",
        "### What you will learn (3–5 bullets)\n",
        "- What makes RL different from supervised learning (and from “just optimisation”).\n",
        "- The core intuition: **actions influence future data**, so learning changes the world you observe.\n",
        "- A first hands-on example: the **exploration vs exploitation** dilemma.\n",
        "\n",
        "### Prereqs\n",
        "- Python basics (functions, loops)\n",
        "- Comfort reading simple plots\n",
        "\n",
        "### Estimated time\n",
        "- 45–60 minutes\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e36a28",
      "metadata": {},
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Reinforcement learning matters now because we are increasingly building systems that don’t just *predict*, they **act**. They make decisions in the world, repeatedly, under uncertainty, with outcomes that arrive late and with feedback that is shaped by their own behavior. In settings like this, the core question isn’t *“what label matches this input?”* but **“what should I do next to achieve a long‑run goal?”** RL is the language and toolbox for that question.\n",
        "\n",
        "A big driver is the rise of **autonomous systems**: software and machines that must operate in dynamic environments, where the “right” action depends on context and where mistakes can compound. An autonomous agent isn’t judged by a single output, but by the quality of an entire trajectory. How it gathers information, how it manages uncertainty, how it recovers from error, and how it balances competing objectives like speed, cost, reliability, and safety. Even when we can train components with supervised learning, the system-level behavior is fundamentally interactive. RL provides a way to directly optimise decision policies against outcomes that only make sense at the level of sequences.\n",
        "\n",
        "**Robotics** makes this concrete. A robot arm learning to grasp, a quadruped learning to walk, or a drone learning to stabilise in wind all face the same reality, that actions change the future state of the world, sensors are noisy, and good control requires reasoning over time. You can’t realistically label the “correct” torque at every millisecond across every possible contact configuration. Instead, you specify outcomes—move efficiently, don’t fall, don’t collide, conserve energy, and learn behavior through interaction (often in simulation first). RL is compelling here not because it’s magic, but because it aligns with how control problems are structured: long horizons, delayed consequences, and the need for policies that adapt.\n",
        "\n",
        "**Healthcare** is another domain where the RL framing is natural, and where it also forces you to confront what matters most, *constraints*. Clinical decisions are sequential. An intervention now can change patient state, influence which options are available later, and affect outcomes that only appear after hours or days. The objective is not next-step accuracy; it’s long-run patient outcomes under uncertainty, with strong safety requirements. RL provides a principled way to talk about sequential treatment policies, long-term return, and the tradeoffs between aggressive actions and risk, while also highlighting that naive reward maximisation is not enough. In many healthcare settings, the most important part of the RL problem is how to encode and enforce safety and how to learn from limited, biased, and ethically constrained data.\n",
        "\n",
        "And then there’s the shift in AI itself: **LLMs and agents**. Modern language models increasingly operate as decision-makers, choosing when to ask a clarifying question, which tool to call, how to plan, and how to allocate computation. These are action choices with consequences, they change what information the system receives next, they incur cost, and they can fail in ways that only become apparent after a sequence of steps. This is exactly the terrain RL was built for: learning policies from interaction and optimising for outcomes that live at the level of trajectories (task success, user satisfaction, safety incidents), not just token-level likelihood.\n",
        "\n",
        "Finally, RL has become central to **LLM training and fine-tuning**. Techniques like **RLHF** (reinforcement learning from human feedback) and related preference-optimisation approaches emerged because we care about properties such as helpfulness, harmlessness, honesty and instruction-following, which are hard to specify as a fixed supervised target for every prompt. Human preferences provide a *reward signal*, and RL provides a framework for turning that signal into improved behavior. Beyond RLHF, there’s increasing interest in more **end-to-end** RL-style training loops where models improve through interactions with environments, with tools, with users, and with automated evaluators. In these settings, the key RL questions reappear in modern form: how to explore safely, how to avoid reward hacking, how to evaluate policies robustly, how to ensure improvements generalise, and how to optimise what we truly want rather than what is easiest to measure.\n",
        "\n",
        "So RL matters now not just because it’s a set of algorithms, but because it matches the shape of the problems we’re increasingly trying to solve. **Learning to act**, under uncertainty, over time, with *real* consequences.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Core Idea\n",
        "\n",
        "### RL in one sentence\n",
        "**Reinforcement learning is learning to make good decisions from interaction.**\n",
        "\n",
        "### The key twist\n",
        "In supervised learning, the dataset is (mostly) fixed. In RL, **your actions determine what data you get next**.\n",
        "\n",
        "That single fact creates most of RL’s “feel”:\n",
        "- You must **explore** to discover better actions.\n",
        "- Exploration can be costly (bad outcomes while you learn).\n",
        "- Rewards are often noisy, delayed, and shaped by your choices.\n",
        "\n",
        "### What RL is *not*\n",
        "- **Not** just prediction: you can predict perfectly and still act poorly.\n",
        "- **Not** just optimization: you optimize *a long-run objective* under feedback and uncertainty.\n",
        "- **Not** always “deep”: classic tabular RL is foundational and still useful.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Multi-armed bandit\n",
        "\n",
        "We’ll start with the smallest possible interactive decision problem: a **multi-armed bandit**.\n",
        "\n",
        "- You repeatedly choose one of several actions (arms).\n",
        "- Each action gives a random reward with an unknown average.\n",
        "- There is no “state” yet. That’s intentional, it isolates the core challenge.\n",
        "\n",
        "Even here, RL-like thinking appears immediately:\n",
        "- If you always pick the best arm *you currently believe in*, you may miss a better one.\n",
        "- If you explore too much, you waste time on worse arms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you're running this in a fresh environment, you only need numpy + matplotlib.\n",
        "# Uncomment below as required:\n",
        "\n",
        "# %pip install numpy\n",
        "# %pip install matplotlib\n",
        "\n",
        "#=================================================================================\n",
        "#=================================================================================\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a random seed for reproducability \n",
        "np.random.seed(7)\n",
        "\n",
        "# Use matplotlib to generate parameters for plots\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
        "plt.rcParams[\"axes.grid\"] = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ba9b3a",
      "metadata": {},
      "source": [
        "## Before we code: what “Gaussian bandit” means\n",
        "\n",
        "In this bandit example, the list `means = [0.2, 0.5, 1.0, 0.7]` does **not** describe one “four-dimensional Gaussian.”\n",
        "Instead, it defines **four separate 1D reward distributions** — one per action (arm):\n",
        "\n",
        "- If you pick arm 0, reward $r \\sim \\mathcal{N}(0.2, \\text{reward\\_std}^2)$\n",
        "- If you pick arm 1, reward $r \\sim \\mathcal{N}(0.5, \\text{reward\\_std}^2)$\n",
        "- If you pick arm 2, reward $r \\sim \\mathcal{N}(1.0, \\text{reward\\_std}^2)$\n",
        "- If you pick arm 3, reward $r \\sim \\mathcal{N}(0.7, \\text{reward\\_std}^2)$\n",
        "\n",
        "So the bandit works like this: **you choose an action**, and then the environment samples a reward from *that arm’s* distribution.\n",
        "There is **no state** and no transition dynamics in this toy problem; just repeated **action → reward**.\n",
        "\n",
        "### Gotchas to keep in mind\n",
        "\n",
        "- **Bandits are “stateless RL”**: this notebook focuses on the exploration–exploitation tradeoff without introducing states yet. (States come in the next notebooks.)\n",
        "- **The reward is noisy**: even a worse arm can occasionally pay out more than the best arm on a single step. That’s why you need multiple samples and why early “greedy” choices can get stuck.\n",
        "- **The best arm is fixed here**: this is a *stationary* bandit (means don’t change). Later we’ll discuss non-stationary settings where the best action can drift.\n",
        "- **Reproducibility**: we set a random seed so your plots are repeatable. If you change the seed (or remove it), the exact curves will change—but the overall pattern should be similar.\n",
        "\n",
        "### The code below initialises the Bandit for the later exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaussianBandit:\n",
        "    \"\"\"\n",
        "    K-armed bandit environment where each arm provides rewards drawn from a Gaussian (Normal) distribution.\n",
        "\n",
        "    Each action (arm) i yields a reward sampled from N(mean_i, reward_std^2).\n",
        "    Bandits are stateless, so the distribution parameters are fixed for each arm during an episode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, means: np.ndarray, reward_std: float = 1.0):\n",
        "        # Initialize the bandit environment.\n",
        "        # Store the means as a numpy array to ensure correct type and operations\n",
        "        self.means = np.array(means, dtype=float)\n",
        "        # Store the standard deviation as float (all arms have the same std dev)\n",
        "        self.reward_std = float(reward_std)\n",
        "\n",
        "    @property\n",
        "    def k(self) -> int:\n",
        "        # Number of arms in the bandit.\n",
        "        return int(self.means.shape[0])\n",
        "\n",
        "    def step(self, action: int) -> float:\n",
        "        #Sample a reward by pulling the specified arm.\n",
        "        return float(np.random.normal(self.means[action], self.reward_std))\n",
        "\n",
        "    def optimal_mean(self) -> float:\n",
        "        # Return the true mean reward of the optimal arm.\n",
        "        return float(np.max(self.means))\n",
        "\n",
        "    def optimal_action(self) -> int:\n",
        "        # Get the index of the optimal (highest-mean) action.\n",
        "        return int(np.argmax(self.means))\n",
        "\n",
        "\n",
        "# Example usage: create a bandit instance with 4 arms and check its parameters\n",
        "bandit = GaussianBandit(means=np.array([0.2, 0.5, 1.0, 0.7]), reward_std=1.0)\n",
        "\n",
        "# Print the true means and the index of the optimal arm\n",
        "bandit.means, bandit.optimal_action()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n",
        "\n",
        "## 3.1 Exercise A\n",
        "\n",
        "We’ll compare three simple strategies:\n",
        "- **Pure exploitation**: always pick the arm with the highest estimated mean.\n",
        "- **ε-greedy**: mostly exploit, but sometimes explore at random.\n",
        "- **Pure exploration**: choose actions uniformly at random.\n",
        "\n",
        "We’ll track:\n",
        "- **Average reward** (how well we’re doing)\n",
        "- **Regret** (how much reward we lost by not always taking the best arm)\n",
        "\n",
        "Regret is a useful lens: it turns “learning efficiency” into a measurable quantity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_epsilon_greedy(\n",
        "    bandit: GaussianBandit,\n",
        "    n_steps: int,\n",
        "    epsilon: float,\n",
        "    initial_value: float = 0.0,\n",
        "):\n",
        "    \"\"\"Run epsilon-greedy with incremental mean estimates.\"\"\"\n",
        "    k = bandit.k\n",
        "    q = np.full(k, initial_value, dtype=float)  # estimated value of each action (Q-values)\n",
        "    n = np.zeros(k, dtype=int)                  # number of times each action has been selected\n",
        "\n",
        "    rewards = np.zeros(n_steps, dtype=float)    # rewards at each step\n",
        "    actions = np.zeros(n_steps, dtype=int)      # actions taken at each step\n",
        "\n",
        "    for t in range(n_steps):\n",
        "        # With probability epsilon, choose a random arm (exploration)\n",
        "        explore = np.random.rand() < epsilon\n",
        "        if explore:\n",
        "            a = np.random.randint(0, k)\n",
        "        else:\n",
        "            # Otherwise, exploit: pick the arm with current highest estimated value\n",
        "            a = int(np.argmax(q))\n",
        "\n",
        "        # Pull the selected arm and observe the reward\n",
        "        r = bandit.step(a)\n",
        "\n",
        "        n[a] += 1  # Update count of times arm a has been selected\n",
        "\n",
        "        # Incremental mean update for Q-value of action a\n",
        "        q[a] += (r - q[a]) / n[a]\n",
        "\n",
        "        # Store the obtained reward and action\n",
        "        rewards[t] = r\n",
        "        actions[t] = a\n",
        "\n",
        "    return rewards, actions\n",
        "\n",
        "\n",
        "def summarise(bandit: GaussianBandit, rewards: np.ndarray):\n",
        "    # Compute the optimal mean reward (for the best possible arm)\n",
        "    optimal = bandit.optimal_mean()\n",
        "    avg_reward = rewards.mean()\n",
        "    regret = (optimal - rewards).cumsum()  # cumulative regret over time\n",
        "    return avg_reward, regret\n",
        "\n",
        "print('This cell has no output! (Except this...)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10c8901",
      "metadata": {},
      "source": [
        "## What this experiment is doing (plain English)\n",
        "\n",
        "We’re going to simulate **$n\\_steps$ interactions** with the bandit.\n",
        "\n",
        "At each time step $t = 1, 2, \\dots, n\\_steps$:\n",
        "\n",
        "1. The agent chooses an **action** (i.e., selects one arm to pull).\n",
        "2. The bandit returns a **reward** by sampling from that arm’s reward distribution.\n",
        "3. The agent updates its **estimate** of how good that arm is (based on the observed reward).\n",
        "\n",
        "We’ll run the *same* bandit under **three different action-selection policies**:\n",
        "\n",
        "- **Exploit-only ($\\epsilon = 0.0$)**: always pull the arm with the highest current estimated value.\n",
        "- **$\\epsilon$-greedy ($\\epsilon = 0.1$)**: usually exploit, but **10%** of the time pick a random arm to explore.\n",
        "- **Random ($\\epsilon = 1.0$)**: always pick a random arm (pure exploration).\n",
        "\n",
        "Then we compare them using two metrics:\n",
        "\n",
        "- **Average reward**: “How much reward did we get on average?”\n",
        "- **Cumulative regret**: “How much reward did we miss out on compared to always pulling the truly best arm?”\n",
        "\n",
        "> Important: even the best arm can give a low reward sometimes (because rewards are noisy), so early on it’s easy for a policy to get fooled. That’s why exploration can help.\n",
        "\n",
        "\n",
        "### Run the below cell multiple times to observe differences in the resulting graphs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of steps in the experiment\n",
        "n_steps = 2000\n",
        "\n",
        "# Run pure exploitation (epsilon=0.0: always greedy)\n",
        "rewards_exploit, _ = run_epsilon_greedy(bandit, n_steps=n_steps, epsilon=0.0)\n",
        "\n",
        "# Run epsilon-greedy: mix of exploration and exploitation\n",
        "rewards_eps, _ = run_epsilon_greedy(bandit, n_steps=n_steps, epsilon=0.1)\n",
        "\n",
        "# Run pure exploration (epsilon=1.0: always random)\n",
        "rewards_random, _ = run_epsilon_greedy(bandit, n_steps=n_steps, epsilon=1.0)\n",
        "\n",
        "# Compute average reward and cumulative regret for each strategy\n",
        "avg_exploit, reg_exploit = summarise(bandit, rewards_exploit)\n",
        "avg_eps, reg_eps = summarise(bandit, rewards_eps)\n",
        "avg_random, reg_random = summarise(bandit, rewards_random)\n",
        "\n",
        "# Print or display the average rewards for each strategy\n",
        "avg_exploit, avg_eps, avg_random\n",
        "\n",
        "\n",
        "def moving_average(x: np.ndarray, window: int = 50) -> np.ndarray:\n",
        "    \"\"\"Compute the moving average of array x with window size 'window'.\"\"\"\n",
        "    x = np.asarray(x)\n",
        "    if window <= 1:\n",
        "        # No smoothing if window is 1 or less\n",
        "        return x\n",
        "    kernel = np.ones(window) / window  # Uniform kernel for convolution\n",
        "    return np.convolve(x, kernel, mode=\"valid\")  # Apply moving average\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 2 subplots side by side for reward and regret curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Left plot: Moving average reward\n",
        "axs[0].plot(moving_average(rewards_exploit), label=f\"exploit only (ε=0.0), avg={avg_exploit:.2f}\")\n",
        "axs[0].plot(moving_average(rewards_eps), label=f\"ε-greedy (ε=0.1), avg={avg_eps:.2f}\")\n",
        "axs[0].plot(moving_average(rewards_random), label=f\"random (ε=1.0), avg={avg_random:.2f}\")\n",
        "axs[0].axhline(bandit.optimal_mean(), color=\"black\", linestyle=\"--\", linewidth=1, label=\"optimal mean\")\n",
        "axs[0].set_title(\"Moving average reward (window=50)\")\n",
        "axs[0].set_xlabel(\"time step\")\n",
        "axs[0].set_ylabel(\"reward\")\n",
        "axs[0].legend()\n",
        "\n",
        "# Right plot: Cumulative regret\n",
        "axs[1].plot(reg_exploit, label=\"exploit only\")\n",
        "axs[1].plot(reg_eps, label=\"ε-greedy\")\n",
        "axs[1].plot(reg_random, label=\"random\")\n",
        "axs[1].set_title(\"Cumulative regret\")\n",
        "axs[1].set_xlabel(\"time step\")\n",
        "axs[1].set_ylabel(\"regret\")\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What you should notice\n",
        "- **Exploit-only can get stuck** early if it commits to a suboptimal arm.\n",
        "- **Random explores too much** and leaves reward on the table.\n",
        "- **A little exploration** often wins: it pays a short-term cost to learn a better long-term behavior.\n",
        "\n",
        "This is the “hello world” of RL: *acting to learn*.\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Exercise B: implement the update that makes the agent **learn**\n",
        "\n",
        "So far, the agent is doing two things:\n",
        "1. **Choosing actions** (which arm to pull)\n",
        "2. **Updating its beliefs** about each arm’s value\n",
        "\n",
        "In this exercise, you’ll implement the *belief update* step.\n",
        "\n",
        "### Goal\n",
        "Fill in the function `incremental_mean_update(...)` so that after the agent pulls arm $a$ and observes reward $r$, it updates its estimate $Q(a)$.\n",
        "\n",
        "### The update rule\n",
        "After you have pulled arm $a$ a total of $N(a)$ times, update:\n",
        "\n",
        "$$\n",
        "Q(a) \\leftarrow Q(a) + \\frac{1}{N(a)}(r - Q(a))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $Q(a)$ is the agent’s current estimate of the **average reward** for arm $a$\n",
        "- $N(a)$ is **how many times you’ve pulled arm $a$ so far** (including this step)\n",
        "- $r$ is the reward you just observed\n",
        "\n",
        "### Intuition (why this makes sense)\n",
        "- The term $(r - Q(a))$ is the **error** between what you observed and what you expected.\n",
        "- The factor $\\frac{1}{N(a)}$ makes updates **smaller over time** as you collect more data for that arm (because your estimate becomes more stable).\n",
        "\n",
        "### Quick sanity check\n",
        "Once implemented, `run_epsilon_greedy_v2(...)` should behave similarly to the earlier version of ε-greedy (average reward in the same ballpark, and it should learn to favor the best arm).\n",
        "\n",
        "#### (Check your answer at the bottom of the notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def incremental_mean_update(q_a: float, n_a: int, r: float) -> float:\n",
        "    \"\"\"Return updated estimate for action a after observing reward r.\n",
        "\n",
        "    q_a: current estimate\n",
        "    n_a: *new* count after incrementing (so n_a >= 1)\n",
        "    r: observed reward\n",
        "    \"\"\"\n",
        "    # TODO: implement incremental mean update\n",
        "    # Add your code here:\n",
        "    # ........................................\n",
        "    # ........................................\n",
        "    # ........................................\n",
        "    # ........................................\n",
        "    return q_a\n",
        "\n",
        "\n",
        "def run_epsilon_greedy_v2(\n",
        "    bandit: GaussianBandit,\n",
        "    n_steps: int,\n",
        "    epsilon: float,\n",
        "    initial_value: float = 0.0,\n",
        "):\n",
        "    k = bandit.k\n",
        "    q = np.full(k, initial_value, dtype=float)\n",
        "    n = np.zeros(k, dtype=int)\n",
        "\n",
        "    rewards = np.zeros(n_steps, dtype=float)\n",
        "    for t in range(n_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "            a = np.random.randint(0, k)\n",
        "        else:\n",
        "            a = int(np.argmax(q))\n",
        "\n",
        "        r = bandit.step(a)\n",
        "        n[a] += 1\n",
        "        q[a] = incremental_mean_update(q[a], n[a], r)\n",
        "        rewards[t] = r\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "# Sanity check: once you implement incremental_mean_update,\n",
        "# this should behave similarly to the earlier implementation.\n",
        "rewards_eps_v2 = run_epsilon_greedy_v2(bandit, n_steps=2000, epsilon=0.1)\n",
        "rewards_eps_v2.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a27ee93",
      "metadata": {
        "vscode": {
          "languageId": "bat"
        }
      },
      "source": [
        "<hr>\n",
        "\n",
        "## Visual intuition: what learning looks like\n",
        "\n",
        "Now that `incremental_mean_update` works, we can *see* learning more directly than by plotting noisy rewards.\n",
        "\n",
        "We’ll plot two things:\n",
        "\n",
        "1) **How often the agent chooses the optimal arm** (rolling window).\n",
        "   - This shows whether the policy is actually improving.\n",
        "\n",
        "2) The agent’s **value estimates** $Q(a)$ for each arm over time.\n",
        "   - This shows what the update rule is doing internally.\n",
        "   - We’ll also draw the **true means** as reference lines.\n",
        "\n",
        "Because rewards are noisy, these plots are usually much easier to interpret than raw reward curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5319a132",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def rolling_mean(x: np.ndarray, window: int = 200) -> np.ndarray:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if window <= 1:\n",
        "        return x\n",
        "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")\n",
        "\n",
        "def trace_epsilon_greedy(\n",
        "    bandit,\n",
        "    n_steps: int,\n",
        "    epsilon: float,\n",
        "    initial_value: float = 0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs epsilon-greedy while recording:\n",
        "    - rewards[t]\n",
        "    - actions[t]\n",
        "    - q_history[t, a]  (the full Q vector over time)\n",
        "    \"\"\"\n",
        "    k = bandit.k\n",
        "    q = np.full(k, initial_value, dtype=float)\n",
        "    n = np.zeros(k, dtype=int)\n",
        "\n",
        "    rewards = np.zeros(n_steps, dtype=float)\n",
        "    actions = np.zeros(n_steps, dtype=int)\n",
        "    q_history = np.zeros((n_steps, k), dtype=float)\n",
        "\n",
        "    for t in range(n_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "            a = np.random.randint(0, k)\n",
        "        else:\n",
        "            a = int(np.argmax(q))\n",
        "\n",
        "        r = bandit.step(a)\n",
        "        n[a] += 1\n",
        "        q[a] = incremental_mean_update(q[a], n[a], r)\n",
        "\n",
        "        rewards[t] = r\n",
        "        actions[t] = a\n",
        "        q_history[t] = q\n",
        "\n",
        "    return rewards, actions, q_history\n",
        "\n",
        "# --- Run experiments (fresh bandits so comparisons are fair) ---\n",
        "n_steps = 4000\n",
        "window = 200\n",
        "\n",
        "np.random.seed(7)\n",
        "bandit0 = GaussianBandit(means=np.array([0.2, 0.5, 1.0, 0.7]), reward_std=1.0)\n",
        "opt_a0 = bandit0.optimal_action()\n",
        "_, actions_exploit, _ = trace_epsilon_greedy(bandit0, n_steps, epsilon=0.0)\n",
        "\n",
        "np.random.seed(7)\n",
        "bandit1 = GaussianBandit(means=np.array([0.2, 0.5, 1.0, 0.7]), reward_std=1.0)\n",
        "opt_a1 = bandit1.optimal_action()\n",
        "_, actions_eps, q_hist_eps = trace_epsilon_greedy(bandit1, n_steps, epsilon=0.1)\n",
        "\n",
        "np.random.seed(7)\n",
        "bandit2 = GaussianBandit(means=np.array([0.2, 0.5, 1.0, 0.7]), reward_std=1.0)\n",
        "opt_a2 = bandit2.optimal_action()\n",
        "_, actions_random, _ = trace_epsilon_greedy(bandit2, n_steps, epsilon=1.0)\n",
        "\n",
        "# --- Plot 1: rolling optimal-action rate ---\n",
        "opt_rate_exploit = rolling_mean(actions_exploit == opt_a0, window=window)\n",
        "opt_rate_eps = rolling_mean(actions_eps == opt_a1, window=window)\n",
        "opt_rate_random = rolling_mean(actions_random == opt_a2, window=window)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(opt_rate_exploit, label=\"exploit-only (ε=0.0)\")\n",
        "plt.plot(opt_rate_eps, label=\"ε-greedy (ε=0.1)\")\n",
        "plt.plot(opt_rate_random, label=\"random (ε=1.0)\")\n",
        "plt.ylim(0, 1.05)\n",
        "plt.title(f\"How often we choose the optimal arm (rolling window={window})\")\n",
        "plt.xlabel(\"time step\")\n",
        "plt.ylabel(\"fraction optimal\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Plot 2: Q(a) estimates over time for ε-greedy ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "for a in range(bandit1.k):\n",
        "    plt.plot(rolling_mean(q_hist_eps[:, a], window=window), label=f\"Q({a}) estimate\")\n",
        "    plt.axhline(bandit1.means[a], linestyle=\"--\", linewidth=1, alpha=0.6)\n",
        "\n",
        "plt.title(f\"Value estimates Q(a) over time (ε=0.1, rolling window={window})\")\n",
        "plt.xlabel(\"time step\")\n",
        "plt.ylabel(\"Q estimate (smoothed)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3646f0a5",
      "metadata": {},
      "source": [
        "<hr>\n",
        "\n",
        "## 4. Diving Deeper\n",
        "\n",
        "This section adds a bit more mathematical structure. You can skip it without affecting your ability to follow the rest of the notebook.\n",
        "\n",
        "### Bandits, formally\n",
        "\n",
        "A **$K$-armed bandit** is one of the simplest RL settings:\n",
        "\n",
        "- There are $K$ actions (arms): $a \\in \\{1,\\dots,K\\}$.\n",
        "- Each arm $a$ has an unknown reward distribution $P_a$ with unknown mean $\\mu_a = \\mathbb{E}[R \\mid a]$.\n",
        "- At each time step $t$, you choose an arm $A_t$ and observe a reward $R_t \\sim P_{A_t}$.\n",
        "- The environment is **stateless** and (in this notebook) **stationary**: the distributions $P_a$ do not change over time.\n",
        "\n",
        "The learning problem is: choose actions to make the total reward large over time.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border: none; border-top: 2px dashed #999; margin: 24px 0;\">\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Expected reward and (pseudo-)regret\n",
        "\n",
        "If you somehow knew the best arm in advance, you would always choose\n",
        "\n",
        "$$\n",
        "a^* = \\arg\\max_a \\mu_a\n",
        "\\quad\\text{with}\\quad\n",
        "\\mu^* = \\max_a \\mu_a.\n",
        "$$\n",
        "\n",
        "To measure how costly learning is, we often use **regret**.\n",
        "\n",
        "There are two closely related versions:\n",
        "\n",
        "1) **Realised regret** (uses the random observed rewards):\n",
        "$$\n",
        "R_T = \\sum_{t=1}^T (\\mu^* - R_t)\n",
        "$$\n",
        "This can sometimes decrease on individual steps because rewards are noisy (you can get $R_t > \\mu^*$ by luck).\n",
        "\n",
        "2) **Pseudo-regret** (uses the true mean of the chosen arm):\n",
        "$$\n",
        "\\bar{R}_T = \\sum_{t=1}^T (\\mu^* - \\mu_{A_t})\n",
        "$$\n",
        "This is always $\\ge 0$ and is often easier to reason about theoretically.\n",
        "\n",
        "Both capture the same idea: **every time you don’t choose the best arm, you pay an opportunity cost.**\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border: none; border-top: 2px dashed #999; margin: 24px 0;\">\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Why incremental updates show up everywhere\n",
        "\n",
        "A basic idea in RL is to maintain an estimate $Q(a)$ of how good action $a$ is, and then improve it using new data.\n",
        "\n",
        "In bandits, a natural target for $Q(a)$ is the action’s mean reward:\n",
        "$$\n",
        "Q(a) \\approx \\mu_a.\n",
        "$$\n",
        "\n",
        "If $Q_n$ is the average of the first $n$ rewards observed for a particular arm, then:\n",
        "$$\n",
        "Q_n = \\frac{1}{n}\\sum_{i=1}^n R_i\n",
        "$$\n",
        "\n",
        "From this definition you can derive the **online (incremental) mean update**:\n",
        "$$\n",
        "Q_n = Q_{n-1} + \\frac{1}{n}(R_n - Q_{n-1}).\n",
        "$$\n",
        "\n",
        "This form is extremely important because it has the “RL update shape” you’ll see all course:\n",
        "- **new estimate = old estimate + step-size × (target − old estimate)**\n",
        "\n",
        "Here the target is the newest reward $R_n$, and the step-size is $\\frac{1}{n}$.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border: none; border-top: 2px dashed #999; margin: 24px 0;\">\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "### Connecting back to exploration vs exploitation\n",
        "\n",
        "Even if you use perfect averaging, you still face a decision problem:\n",
        "- **Exploit**: choose the arm with the highest current estimate $Q(a)$\n",
        "- **Explore**: try something uncertain to improve your estimates\n",
        "\n",
        "ε-greedy is one of the simplest strategies for balancing this tradeoff.\n",
        "\n",
        "\n",
        "### If you want to go deeper\n",
        "\n",
        "- Sutton & Barto (2018), Chapter 2 (Multi-armed bandits)\n",
        "- Lattimore & Szepesvári, *Bandit Algorithms* (more formal regret bounds and methods like UCB/Thompson Sampling)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Knowledge Check\n",
        "\n",
        "### Concept Checks:\n",
        "1) In your own words, why does exploration change the *data distribution* you learn from?\n",
        "2) Why can “exploit-only” be a bad strategy even when the world is stationary?\n",
        "3) What does cumulative regret measure?\n",
        "\n",
        "### Small coding change:\n",
        "- Make the bandit harder by increasing `reward_std` to 2.0. What happens to regret? Why?\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Takeaways\n",
        "- RL is about **sequential decision-making** with feedback.\n",
        "- Actions influence the data you get, so learning and control are coupled.\n",
        "- Even the simplest setting forces the exploration–exploitation tradeoff.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## 8. What’s Next\n",
        "- Next notebook: **RL loop + vocabulary** (state, action, reward, return, policy, value).\n",
        "- Suggested reading: Sutton & Barto (2018), Chapter 2 (Bandits) for additional intuition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0279e376",
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################################################\n",
        "###################### ANSWERS BELOW #################################\n",
        "######################################################################\n",
        "###################### ANSWERS BELOW #################################\n",
        "######################################################################\n",
        "###################### ANSWERS BELOW #################################\n",
        "######################################################################\n",
        "            \n",
        "                           #####\n",
        "                           #####                                                 \n",
        "                           #####                                                  \n",
        "                           #####                                                    \n",
        "                           #####                                                    \n",
        "                        ###########                     \n",
        "                         #########                       \n",
        "                          #######                           \n",
        "                           #####                             \n",
        "                            ###                              \n",
        "                             #                                \n",
        "              "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbc6621",
      "metadata": {},
      "source": [
        "## Answer to Exersise B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85b2577",
      "metadata": {},
      "outputs": [],
      "source": [
        "def incremental_mean_update(q_a: float, n_a: int, r: float) -> float:\n",
        "    \"\"\"Return updated estimate for action a after observing reward r.\n",
        "\n",
        "    q_a: current estimate\n",
        "    n_a: *new* count after incrementing (so n_a >= 1)\n",
        "    r: observed reward\n",
        "    \"\"\"\n",
        "    # This is the added line of code that reflects the given update formula\n",
        "    q_a = q_a + 1/n_a * (r - q_a) # <--------------------------------------\n",
        "    return q_a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b34c495",
      "metadata": {},
      "source": [
        "## Answers to Knowledge Check:\n",
        "<hr style=\"border: none; border-top: 2px dashed #999; margin: 24px 0;\">\n",
        "\n",
        "### Concept checks: \n",
        "\n",
        "**1. Why does exploration change the data distribution you learn from?**\n",
        "\n",
        "    Because in RL the data you collect depends on your actions. If you explore, you deliberately take actions you wouldn’t otherwise take, so you observe rewards/states from parts of the environment you’d miss under a purely greedy policy. Different policies ⇒ different action frequencies ⇒ different samples ⇒ different training data.\n",
        "\n",
        "**2. Why can exploit-only be bad even when the world is stationary?**\n",
        "\n",
        "    Early on your value estimates are noisy and based on few samples. If you exploit-only, you can “lock in” to an arm that looked best due to randomness and never gather enough evidence to discover a truly better arm. Stationary doesn’t mean “obvious from one sample”—it just means the underlying means don’t change.\n",
        "\n",
        "**3. What does cumulative regret measure?**\n",
        "\n",
        "    It measures the total opportunity cost of learning: how much reward you lost over time compared to an oracle that always picks the optimal arm. Lower cumulative regret means you learned faster / made better decisions sooner.\n",
        "\n",
        "### Small coding change:\n",
        "\n",
        "**- Increase reward_std to 2.0: what happens to regret, and why?**\n",
        "\n",
        "    Regret typically increases (grows faster), because rewards become noisier. Noisier feedback makes it harder to tell which arm is truly best, so the agent needs more samples (and makes more mistakes) before it confidently exploits the optimal arm. You’ll often see slower improvement and more variability across runs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
